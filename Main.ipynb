{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,\n",
    "    transform = ToTensor(),\n",
    "    download = True\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = False,\n",
    "    transform = ToTensor(),\n",
    "    download = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4,  ..., 5, 6, 8])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.targets # dataset.target shows the lable of the classified data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a DataLoader ?\n",
    "\n",
    "In PyTorch, a DataLoader is a utility class that simplifies the process of loading and iterating over datasets while training deep learning models. Here are some key features of DataLoader:\n",
    "\n",
    "**Batching:** It can divide the dataset into smaller batches, which helps in efficient training.\n",
    "\n",
    "**Shuffling:** It can shuffle the data to ensure that the model does not learn any unintended patterns.\n",
    "\n",
    "**Parallel Processing:** It can load data in parallel using multiple workers, which speeds up the data loading process.\n",
    "\n",
    "**Memory Pinning:** It can pin memory to improve the efficiency of data transfer to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loaders = {\n",
    "    'train': DataLoader(train_data,batch_size=100,shuffle=True,num_workers=1),\n",
    "    'test': DataLoader(test_data,batch_size=100,shuffle=True,num_workers=1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x1ccb41a27e0>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x1ccb2597920>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is nn.Model ?\n",
    "\n",
    "The nn.Module parameter is actually quite important in PyTorch. When you define a neural network class, it should inherit from nn.Module. This inheritance provides your class with a lot of useful methods and attributes that are essential for building and training neural networks.\n",
    "\n",
    "**Initialization and Structure:** By inheriting from nn.Module, your class gains the ability to register layers (like conv1, conv2, etc.) and parameters. This is crucial for PyTorch to keep track of the model’s parameters and their gradients.\n",
    "\n",
    "**Forward Propagation:** The forward method is a special method in nn.Module that defines the forward pass of the network. When you call your model on an input, it automatically calls the forward method.\n",
    "\n",
    "**Model Management:** Inheriting from nn.Module allows you to use various utility functions provided by PyTorch, such as model.eval(), model.train(), model.to(device), and more. These functions are essential for managing the model’s state and moving it between devices (CPU/GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1,10,kernel_size=5) # 1 input channel， 10 output channel, 5x5 kernel, 10 kernels\n",
    "        self.conv2 = nn.Conv2d(10,20,kernel_size=5) # 20 kernels\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320,50)\n",
    "        self.fc2 = nn.Linear(50,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x),2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)),2))\n",
    "        x = x.view(-1,320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.softmax(x, dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.First Convolutional Layer (conv1):**\n",
    "- Input: 1 channel, 28x28 (assuming MNIST dataset)\n",
    "- Output: 10 channels, 24x24 (since kernel size is 5, the output size is reduced by 4)\n",
    "\n",
    "**2.First Max Pooling Layer:**\n",
    "- Output: 10 channels, 12x12 (pooling with kernel size 2 reduces each dimension by half)\n",
    "\n",
    "**3.Second Convolutional Layer (conv2):**\n",
    "- Input: 10 channels, 12x12\n",
    "- Output: 20 channels, 8x8 (again, kernel size 5 reduces each dimension by 4)\n",
    "\n",
    "**4.Second Max Pooling Layer:**\n",
    "- Output: 20 channels, 4x4 (pooling with kernel size 2 reduces each dimension by half)\n",
    "\n",
    "After these layers, the feature map has dimensions 20 channels x 4 height x 4 width. When you flatten this for the fully connected layer, you get (20 \\times 4 \\times 4 = 320) units.\n",
    "\n",
    "So, the input to the first fully connected layer (fc1) is 320 units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape:\n",
    "\n",
    "`x = x.view(-1,320)`  reshapes the output tensor from the second max pooling layer. The original tensor has a shape of **\\[batch_size, channels, height, width\\]**, after it became **\\[batch_size, 320\\]**. This is called flattening tensor. It is necessary because fully connected layers expect 2D input **\\[size, features\\]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax:\n",
    "\n",
    "`F.softmax(x, dim=1)` This is essential when dealing with a classification problem. We use this function to turn the output result into probabilities and make their sum-up result to 1, then we can identify the prediction as the output with the highest probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2.4.1+cu124\n",
      "12.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # connect to gpu\n",
    "\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)                                            # Initializes the CNN model and moves it to the specified device (GPU or CPU).\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)                # Initializes the Adam optimizer with the model’s parameters and a learning rate of 0.001.\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()                                     # Sets the loss function to CrossEntropyLoss\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()                                                   # Sets the model to training mode.\n",
    "    for batch_index, (data,target) in enumerate(loaders['train']):  # Iterates over the training data.\n",
    "        data,target = data.to(device), target.to(device)            # Moves the data and target to the specified device.\n",
    "        optimizer.zero_grad()                                       # Clears the gradients of all optimized tensors.\n",
    "        output = model(data)                                        # Passes the data through the model to get the output.\n",
    "        loss = loss_fn(output,target)                               # Computes the loss between the output and the target.\n",
    "        loss.backward()                                             # Backpropagates the loss.\n",
    "        optimizer.step()                                            # Updates the model parameters.\n",
    "        if batch_index % 20 == 0:                                   # Prints the training progress every 20 batches.\n",
    "            print(f'Train Epoch: {epoch} [{batch_index*len(data)}/{len(loaders[\"train\"].dataset)} ({100.*batch_index/len(loaders[\"train\"]):.0f}%)]\\t{loss.item():.6f}')\n",
    "\n",
    "def test():\n",
    "    model.eval()                                                    # Sets the model to evaluation mode.\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0                                                     # Initializes the test loss and correct predictions counters.\n",
    "\n",
    "    with torch.no_grad():                                           # Disables gradient calculation for efficiency.\n",
    "        for data, target in loaders['test']:                        # Iterates over the test data.\n",
    "            data, target = data.to(device), target.to(device)       # Moves the data and target to the specified device.\n",
    "            output = model(data)                                    # Passes the data through the model to get the output.\n",
    "            test_loss += loss_fn(output,target).item()              # Accumulates the test loss.\n",
    "            pred = output.argmax(dim=1, keepdim=True)               # Gets the predicted class.\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()   # Counts the correct predictions.\n",
    "\n",
    "    test_loss /= len(loaders['test'].dataset)                       # Averages the test loss.\n",
    "    print(f'\\nTest set: Average loss {test_loss:.4f}, Accuracy {correct}/{len(loaders[\"test\"].dataset)}({100.*correct/len(loaders[\"test\"].dataset):.0f}%\\n)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\t2.301164\n",
      "Train Epoch: 1 [2000/60000 (3%)]\t2.292235\n",
      "Train Epoch: 1 [4000/60000 (7%)]\t2.207031\n",
      "Train Epoch: 1 [6000/60000 (10%)]\t2.001616\n",
      "Train Epoch: 1 [8000/60000 (13%)]\t2.000406\n",
      "Train Epoch: 1 [10000/60000 (17%)]\t1.915542\n",
      "Train Epoch: 1 [12000/60000 (20%)]\t1.905369\n",
      "Train Epoch: 1 [14000/60000 (23%)]\t1.825327\n",
      "Train Epoch: 1 [16000/60000 (27%)]\t1.852719\n",
      "Train Epoch: 1 [18000/60000 (30%)]\t1.852872\n",
      "Train Epoch: 1 [20000/60000 (33%)]\t1.745230\n",
      "Train Epoch: 1 [22000/60000 (37%)]\t1.717843\n",
      "Train Epoch: 1 [24000/60000 (40%)]\t1.732881\n",
      "Train Epoch: 1 [26000/60000 (43%)]\t1.763530\n",
      "Train Epoch: 1 [28000/60000 (47%)]\t1.669559\n",
      "Train Epoch: 1 [30000/60000 (50%)]\t1.736648\n",
      "Train Epoch: 1 [32000/60000 (53%)]\t1.685609\n",
      "Train Epoch: 1 [34000/60000 (57%)]\t1.682376\n",
      "Train Epoch: 1 [36000/60000 (60%)]\t1.632298\n",
      "Train Epoch: 1 [38000/60000 (63%)]\t1.618492\n",
      "Train Epoch: 1 [40000/60000 (67%)]\t1.761797\n",
      "Train Epoch: 1 [42000/60000 (70%)]\t1.650318\n",
      "Train Epoch: 1 [44000/60000 (73%)]\t1.619906\n",
      "Train Epoch: 1 [46000/60000 (77%)]\t1.626528\n",
      "Train Epoch: 1 [48000/60000 (80%)]\t1.659707\n",
      "Train Epoch: 1 [50000/60000 (83%)]\t1.643196\n",
      "Train Epoch: 1 [52000/60000 (87%)]\t1.632179\n",
      "Train Epoch: 1 [54000/60000 (90%)]\t1.565397\n",
      "Train Epoch: 1 [56000/60000 (93%)]\t1.687709\n",
      "Train Epoch: 1 [58000/60000 (97%)]\t1.630020\n",
      "\n",
      "Test set: Average loss 0.0154, Accuracy 9258/10000(93%\n",
      ")\n",
      "Train Epoch: 2 [0/60000 (0%)]\t1.659416\n",
      "Train Epoch: 2 [2000/60000 (3%)]\t1.616637\n",
      "Train Epoch: 2 [4000/60000 (7%)]\t1.560400\n",
      "Train Epoch: 2 [6000/60000 (10%)]\t1.669912\n",
      "Train Epoch: 2 [8000/60000 (13%)]\t1.654275\n",
      "Train Epoch: 2 [10000/60000 (17%)]\t1.550965\n",
      "Train Epoch: 2 [12000/60000 (20%)]\t1.645287\n",
      "Train Epoch: 2 [14000/60000 (23%)]\t1.660727\n",
      "Train Epoch: 2 [16000/60000 (27%)]\t1.629244\n",
      "Train Epoch: 2 [18000/60000 (30%)]\t1.636726\n",
      "Train Epoch: 2 [20000/60000 (33%)]\t1.634481\n",
      "Train Epoch: 2 [22000/60000 (37%)]\t1.586266\n",
      "Train Epoch: 2 [24000/60000 (40%)]\t1.592134\n",
      "Train Epoch: 2 [26000/60000 (43%)]\t1.612247\n",
      "Train Epoch: 2 [28000/60000 (47%)]\t1.615544\n",
      "Train Epoch: 2 [30000/60000 (50%)]\t1.601216\n",
      "Train Epoch: 2 [32000/60000 (53%)]\t1.591824\n",
      "Train Epoch: 2 [34000/60000 (57%)]\t1.612227\n",
      "Train Epoch: 2 [36000/60000 (60%)]\t1.608951\n",
      "Train Epoch: 2 [38000/60000 (63%)]\t1.636467\n",
      "Train Epoch: 2 [40000/60000 (67%)]\t1.588742\n",
      "Train Epoch: 2 [42000/60000 (70%)]\t1.605543\n",
      "Train Epoch: 2 [44000/60000 (73%)]\t1.573956\n",
      "Train Epoch: 2 [46000/60000 (77%)]\t1.563126\n",
      "Train Epoch: 2 [48000/60000 (80%)]\t1.604251\n",
      "Train Epoch: 2 [50000/60000 (83%)]\t1.570597\n",
      "Train Epoch: 2 [52000/60000 (87%)]\t1.564706\n",
      "Train Epoch: 2 [54000/60000 (90%)]\t1.619869\n",
      "Train Epoch: 2 [56000/60000 (93%)]\t1.580788\n",
      "Train Epoch: 2 [58000/60000 (97%)]\t1.562913\n",
      "\n",
      "Test set: Average loss 0.0151, Accuracy 9527/10000(95%\n",
      ")\n",
      "Train Epoch: 3 [0/60000 (0%)]\t1.584201\n",
      "Train Epoch: 3 [2000/60000 (3%)]\t1.556237\n",
      "Train Epoch: 3 [4000/60000 (7%)]\t1.561220\n",
      "Train Epoch: 3 [6000/60000 (10%)]\t1.494751\n",
      "Train Epoch: 3 [8000/60000 (13%)]\t1.652522\n",
      "Train Epoch: 3 [10000/60000 (17%)]\t1.607273\n",
      "Train Epoch: 3 [12000/60000 (20%)]\t1.594650\n",
      "Train Epoch: 3 [14000/60000 (23%)]\t1.591348\n",
      "Train Epoch: 3 [16000/60000 (27%)]\t1.587364\n",
      "Train Epoch: 3 [18000/60000 (30%)]\t1.543796\n",
      "Train Epoch: 3 [20000/60000 (33%)]\t1.572251\n",
      "Train Epoch: 3 [22000/60000 (37%)]\t1.582016\n",
      "Train Epoch: 3 [24000/60000 (40%)]\t1.625108\n",
      "Train Epoch: 3 [26000/60000 (43%)]\t1.612930\n",
      "Train Epoch: 3 [28000/60000 (47%)]\t1.616659\n",
      "Train Epoch: 3 [30000/60000 (50%)]\t1.605084\n",
      "Train Epoch: 3 [32000/60000 (53%)]\t1.502711\n",
      "Train Epoch: 3 [34000/60000 (57%)]\t1.540905\n",
      "Train Epoch: 3 [36000/60000 (60%)]\t1.574945\n",
      "Train Epoch: 3 [38000/60000 (63%)]\t1.553182\n",
      "Train Epoch: 3 [40000/60000 (67%)]\t1.553189\n",
      "Train Epoch: 3 [42000/60000 (70%)]\t1.610866\n",
      "Train Epoch: 3 [44000/60000 (73%)]\t1.559668\n",
      "Train Epoch: 3 [46000/60000 (77%)]\t1.605676\n",
      "Train Epoch: 3 [48000/60000 (80%)]\t1.556856\n",
      "Train Epoch: 3 [50000/60000 (83%)]\t1.572714\n",
      "Train Epoch: 3 [52000/60000 (87%)]\t1.570791\n",
      "Train Epoch: 3 [54000/60000 (90%)]\t1.583994\n",
      "Train Epoch: 3 [56000/60000 (93%)]\t1.552013\n",
      "Train Epoch: 3 [58000/60000 (97%)]\t1.584329\n",
      "\n",
      "Test set: Average loss 0.0150, Accuracy 9577/10000(96%\n",
      ")\n",
      "Train Epoch: 4 [0/60000 (0%)]\t1.509808\n",
      "Train Epoch: 4 [2000/60000 (3%)]\t1.540500\n",
      "Train Epoch: 4 [4000/60000 (7%)]\t1.569674\n",
      "Train Epoch: 4 [6000/60000 (10%)]\t1.548068\n",
      "Train Epoch: 4 [8000/60000 (13%)]\t1.554612\n",
      "Train Epoch: 4 [10000/60000 (17%)]\t1.551712\n",
      "Train Epoch: 4 [12000/60000 (20%)]\t1.540172\n",
      "Train Epoch: 4 [14000/60000 (23%)]\t1.548387\n",
      "Train Epoch: 4 [16000/60000 (27%)]\t1.523013\n",
      "Train Epoch: 4 [18000/60000 (30%)]\t1.599041\n",
      "Train Epoch: 4 [20000/60000 (33%)]\t1.560958\n",
      "Train Epoch: 4 [22000/60000 (37%)]\t1.558071\n",
      "Train Epoch: 4 [24000/60000 (40%)]\t1.573277\n",
      "Train Epoch: 4 [26000/60000 (43%)]\t1.558546\n",
      "Train Epoch: 4 [28000/60000 (47%)]\t1.526556\n",
      "Train Epoch: 4 [30000/60000 (50%)]\t1.526238\n",
      "Train Epoch: 4 [32000/60000 (53%)]\t1.570206\n",
      "Train Epoch: 4 [34000/60000 (57%)]\t1.636222\n",
      "Train Epoch: 4 [36000/60000 (60%)]\t1.530619\n",
      "Train Epoch: 4 [38000/60000 (63%)]\t1.586373\n",
      "Train Epoch: 4 [40000/60000 (67%)]\t1.557036\n",
      "Train Epoch: 4 [42000/60000 (70%)]\t1.524522\n",
      "Train Epoch: 4 [44000/60000 (73%)]\t1.563338\n",
      "Train Epoch: 4 [46000/60000 (77%)]\t1.565527\n",
      "Train Epoch: 4 [48000/60000 (80%)]\t1.596755\n",
      "Train Epoch: 4 [50000/60000 (83%)]\t1.547710\n",
      "Train Epoch: 4 [52000/60000 (87%)]\t1.591877\n",
      "Train Epoch: 4 [54000/60000 (90%)]\t1.547210\n",
      "Train Epoch: 4 [56000/60000 (93%)]\t1.549134\n",
      "Train Epoch: 4 [58000/60000 (97%)]\t1.585221\n",
      "\n",
      "Test set: Average loss 0.0150, Accuracy 9635/10000(96%\n",
      ")\n",
      "Train Epoch: 5 [0/60000 (0%)]\t1.524252\n",
      "Train Epoch: 5 [2000/60000 (3%)]\t1.569483\n",
      "Train Epoch: 5 [4000/60000 (7%)]\t1.537775\n",
      "Train Epoch: 5 [6000/60000 (10%)]\t1.547397\n",
      "Train Epoch: 5 [8000/60000 (13%)]\t1.554839\n",
      "Train Epoch: 5 [10000/60000 (17%)]\t1.549679\n",
      "Train Epoch: 5 [12000/60000 (20%)]\t1.550447\n",
      "Train Epoch: 5 [14000/60000 (23%)]\t1.542661\n",
      "Train Epoch: 5 [16000/60000 (27%)]\t1.541855\n",
      "Train Epoch: 5 [18000/60000 (30%)]\t1.527266\n",
      "Train Epoch: 5 [20000/60000 (33%)]\t1.529825\n",
      "Train Epoch: 5 [22000/60000 (37%)]\t1.512566\n",
      "Train Epoch: 5 [24000/60000 (40%)]\t1.514871\n",
      "Train Epoch: 5 [26000/60000 (43%)]\t1.532021\n",
      "Train Epoch: 5 [28000/60000 (47%)]\t1.609789\n",
      "Train Epoch: 5 [30000/60000 (50%)]\t1.530317\n",
      "Train Epoch: 5 [32000/60000 (53%)]\t1.536311\n",
      "Train Epoch: 5 [34000/60000 (57%)]\t1.594275\n",
      "Train Epoch: 5 [36000/60000 (60%)]\t1.534083\n",
      "Train Epoch: 5 [38000/60000 (63%)]\t1.514875\n",
      "Train Epoch: 5 [40000/60000 (67%)]\t1.563101\n",
      "Train Epoch: 5 [42000/60000 (70%)]\t1.548227\n",
      "Train Epoch: 5 [44000/60000 (73%)]\t1.573021\n",
      "Train Epoch: 5 [46000/60000 (77%)]\t1.570653\n",
      "Train Epoch: 5 [48000/60000 (80%)]\t1.562703\n",
      "Train Epoch: 5 [50000/60000 (83%)]\t1.534292\n",
      "Train Epoch: 5 [52000/60000 (87%)]\t1.563854\n",
      "Train Epoch: 5 [54000/60000 (90%)]\t1.556976\n",
      "Train Epoch: 5 [56000/60000 (93%)]\t1.549696\n",
      "Train Epoch: 5 [58000/60000 (97%)]\t1.554975\n",
      "\n",
      "Test set: Average loss 0.0149, Accuracy 9686/10000(97%\n",
      ")\n",
      "Train Epoch: 6 [0/60000 (0%)]\t1.579926\n",
      "Train Epoch: 6 [2000/60000 (3%)]\t1.573271\n",
      "Train Epoch: 6 [4000/60000 (7%)]\t1.569655\n",
      "Train Epoch: 6 [6000/60000 (10%)]\t1.571565\n",
      "Train Epoch: 6 [8000/60000 (13%)]\t1.593190\n",
      "Train Epoch: 6 [10000/60000 (17%)]\t1.546084\n",
      "Train Epoch: 6 [12000/60000 (20%)]\t1.565952\n",
      "Train Epoch: 6 [14000/60000 (23%)]\t1.552156\n",
      "Train Epoch: 6 [16000/60000 (27%)]\t1.526898\n",
      "Train Epoch: 6 [18000/60000 (30%)]\t1.594727\n",
      "Train Epoch: 6 [20000/60000 (33%)]\t1.543720\n",
      "Train Epoch: 6 [22000/60000 (37%)]\t1.544171\n",
      "Train Epoch: 6 [24000/60000 (40%)]\t1.535593\n",
      "Train Epoch: 6 [26000/60000 (43%)]\t1.527318\n",
      "Train Epoch: 6 [28000/60000 (47%)]\t1.539745\n",
      "Train Epoch: 6 [30000/60000 (50%)]\t1.554744\n",
      "Train Epoch: 6 [32000/60000 (53%)]\t1.586913\n",
      "Train Epoch: 6 [34000/60000 (57%)]\t1.538864\n",
      "Train Epoch: 6 [36000/60000 (60%)]\t1.527505\n",
      "Train Epoch: 6 [38000/60000 (63%)]\t1.526609\n",
      "Train Epoch: 6 [40000/60000 (67%)]\t1.524133\n",
      "Train Epoch: 6 [42000/60000 (70%)]\t1.567399\n",
      "Train Epoch: 6 [44000/60000 (73%)]\t1.595750\n",
      "Train Epoch: 6 [46000/60000 (77%)]\t1.553401\n",
      "Train Epoch: 6 [48000/60000 (80%)]\t1.515570\n",
      "Train Epoch: 6 [50000/60000 (83%)]\t1.536608\n",
      "Train Epoch: 6 [52000/60000 (87%)]\t1.574398\n",
      "Train Epoch: 6 [54000/60000 (90%)]\t1.574703\n",
      "Train Epoch: 6 [56000/60000 (93%)]\t1.508064\n",
      "Train Epoch: 6 [58000/60000 (97%)]\t1.503528\n",
      "\n",
      "Test set: Average loss 0.0149, Accuracy 9717/10000(97%\n",
      ")\n",
      "Train Epoch: 7 [0/60000 (0%)]\t1.566571\n",
      "Train Epoch: 7 [2000/60000 (3%)]\t1.546164\n",
      "Train Epoch: 7 [4000/60000 (7%)]\t1.537650\n",
      "Train Epoch: 7 [6000/60000 (10%)]\t1.514857\n",
      "Train Epoch: 7 [8000/60000 (13%)]\t1.551289\n",
      "Train Epoch: 7 [10000/60000 (17%)]\t1.532596\n",
      "Train Epoch: 7 [12000/60000 (20%)]\t1.651316\n",
      "Train Epoch: 7 [14000/60000 (23%)]\t1.575154\n",
      "Train Epoch: 7 [16000/60000 (27%)]\t1.488739\n",
      "Train Epoch: 7 [18000/60000 (30%)]\t1.555127\n",
      "Train Epoch: 7 [20000/60000 (33%)]\t1.516756\n",
      "Train Epoch: 7 [22000/60000 (37%)]\t1.529058\n",
      "Train Epoch: 7 [24000/60000 (40%)]\t1.524374\n",
      "Train Epoch: 7 [26000/60000 (43%)]\t1.518784\n",
      "Train Epoch: 7 [28000/60000 (47%)]\t1.502857\n",
      "Train Epoch: 7 [30000/60000 (50%)]\t1.502133\n",
      "Train Epoch: 7 [32000/60000 (53%)]\t1.538132\n",
      "Train Epoch: 7 [34000/60000 (57%)]\t1.556883\n",
      "Train Epoch: 7 [36000/60000 (60%)]\t1.540021\n",
      "Train Epoch: 7 [38000/60000 (63%)]\t1.510663\n",
      "Train Epoch: 7 [40000/60000 (67%)]\t1.553650\n",
      "Train Epoch: 7 [42000/60000 (70%)]\t1.560081\n",
      "Train Epoch: 7 [44000/60000 (73%)]\t1.517031\n",
      "Train Epoch: 7 [46000/60000 (77%)]\t1.530483\n",
      "Train Epoch: 7 [48000/60000 (80%)]\t1.515040\n",
      "Train Epoch: 7 [50000/60000 (83%)]\t1.537660\n",
      "Train Epoch: 7 [52000/60000 (87%)]\t1.549768\n",
      "Train Epoch: 7 [54000/60000 (90%)]\t1.546566\n",
      "Train Epoch: 7 [56000/60000 (93%)]\t1.551934\n",
      "Train Epoch: 7 [58000/60000 (97%)]\t1.528739\n",
      "\n",
      "Test set: Average loss 0.0149, Accuracy 9729/10000(97%\n",
      ")\n",
      "Train Epoch: 8 [0/60000 (0%)]\t1.561255\n",
      "Train Epoch: 8 [2000/60000 (3%)]\t1.539973\n",
      "Train Epoch: 8 [4000/60000 (7%)]\t1.520022\n",
      "Train Epoch: 8 [6000/60000 (10%)]\t1.517254\n",
      "Train Epoch: 8 [8000/60000 (13%)]\t1.531033\n",
      "Train Epoch: 8 [10000/60000 (17%)]\t1.570452\n",
      "Train Epoch: 8 [12000/60000 (20%)]\t1.522594\n",
      "Train Epoch: 8 [14000/60000 (23%)]\t1.536908\n",
      "Train Epoch: 8 [16000/60000 (27%)]\t1.482835\n",
      "Train Epoch: 8 [18000/60000 (30%)]\t1.514161\n",
      "Train Epoch: 8 [20000/60000 (33%)]\t1.501244\n",
      "Train Epoch: 8 [22000/60000 (37%)]\t1.544720\n",
      "Train Epoch: 8 [24000/60000 (40%)]\t1.562230\n",
      "Train Epoch: 8 [26000/60000 (43%)]\t1.547930\n",
      "Train Epoch: 8 [28000/60000 (47%)]\t1.534933\n",
      "Train Epoch: 8 [30000/60000 (50%)]\t1.526908\n",
      "Train Epoch: 8 [32000/60000 (53%)]\t1.523526\n",
      "Train Epoch: 8 [34000/60000 (57%)]\t1.546066\n",
      "Train Epoch: 8 [36000/60000 (60%)]\t1.563567\n",
      "Train Epoch: 8 [38000/60000 (63%)]\t1.517053\n",
      "Train Epoch: 8 [40000/60000 (67%)]\t1.524872\n",
      "Train Epoch: 8 [42000/60000 (70%)]\t1.542824\n",
      "Train Epoch: 8 [44000/60000 (73%)]\t1.516087\n",
      "Train Epoch: 8 [46000/60000 (77%)]\t1.528853\n",
      "Train Epoch: 8 [48000/60000 (80%)]\t1.531175\n",
      "Train Epoch: 8 [50000/60000 (83%)]\t1.529493\n",
      "Train Epoch: 8 [52000/60000 (87%)]\t1.558119\n",
      "Train Epoch: 8 [54000/60000 (90%)]\t1.496935\n",
      "Train Epoch: 8 [56000/60000 (93%)]\t1.494559\n",
      "Train Epoch: 8 [58000/60000 (97%)]\t1.529664\n",
      "\n",
      "Test set: Average loss 0.0149, Accuracy 9731/10000(97%\n",
      ")\n",
      "Train Epoch: 9 [0/60000 (0%)]\t1.540082\n",
      "Train Epoch: 9 [2000/60000 (3%)]\t1.610238\n",
      "Train Epoch: 9 [4000/60000 (7%)]\t1.555237\n",
      "Train Epoch: 9 [6000/60000 (10%)]\t1.563376\n",
      "Train Epoch: 9 [8000/60000 (13%)]\t1.537683\n",
      "Train Epoch: 9 [10000/60000 (17%)]\t1.525349\n",
      "Train Epoch: 9 [12000/60000 (20%)]\t1.600462\n",
      "Train Epoch: 9 [14000/60000 (23%)]\t1.531095\n",
      "Train Epoch: 9 [16000/60000 (27%)]\t1.536901\n",
      "Train Epoch: 9 [18000/60000 (30%)]\t1.596508\n",
      "Train Epoch: 9 [20000/60000 (33%)]\t1.570059\n",
      "Train Epoch: 9 [22000/60000 (37%)]\t1.527538\n",
      "Train Epoch: 9 [24000/60000 (40%)]\t1.524134\n",
      "Train Epoch: 9 [26000/60000 (43%)]\t1.539142\n",
      "Train Epoch: 9 [28000/60000 (47%)]\t1.574202\n",
      "Train Epoch: 9 [30000/60000 (50%)]\t1.491539\n",
      "Train Epoch: 9 [32000/60000 (53%)]\t1.580038\n",
      "Train Epoch: 9 [34000/60000 (57%)]\t1.502134\n",
      "Train Epoch: 9 [36000/60000 (60%)]\t1.549390\n",
      "Train Epoch: 9 [38000/60000 (63%)]\t1.555770\n",
      "Train Epoch: 9 [40000/60000 (67%)]\t1.500456\n",
      "Train Epoch: 9 [42000/60000 (70%)]\t1.533506\n",
      "Train Epoch: 9 [44000/60000 (73%)]\t1.545029\n",
      "Train Epoch: 9 [46000/60000 (77%)]\t1.497976\n",
      "Train Epoch: 9 [48000/60000 (80%)]\t1.532119\n",
      "Train Epoch: 9 [50000/60000 (83%)]\t1.530868\n",
      "Train Epoch: 9 [52000/60000 (87%)]\t1.521205\n",
      "Train Epoch: 9 [54000/60000 (90%)]\t1.531168\n",
      "Train Epoch: 9 [56000/60000 (93%)]\t1.491160\n",
      "Train Epoch: 9 [58000/60000 (97%)]\t1.492027\n",
      "\n",
      "Test set: Average loss 0.0149, Accuracy 9746/10000(97%\n",
      ")\n",
      "Train Epoch: 10 [0/60000 (0%)]\t1.485997\n",
      "Train Epoch: 10 [2000/60000 (3%)]\t1.503565\n",
      "Train Epoch: 10 [4000/60000 (7%)]\t1.491367\n",
      "Train Epoch: 10 [6000/60000 (10%)]\t1.539372\n",
      "Train Epoch: 10 [8000/60000 (13%)]\t1.490435\n",
      "Train Epoch: 10 [10000/60000 (17%)]\t1.520850\n",
      "Train Epoch: 10 [12000/60000 (20%)]\t1.552874\n",
      "Train Epoch: 10 [14000/60000 (23%)]\t1.522323\n",
      "Train Epoch: 10 [16000/60000 (27%)]\t1.521940\n",
      "Train Epoch: 10 [18000/60000 (30%)]\t1.506981\n",
      "Train Epoch: 10 [20000/60000 (33%)]\t1.544957\n",
      "Train Epoch: 10 [22000/60000 (37%)]\t1.491777\n",
      "Train Epoch: 10 [24000/60000 (40%)]\t1.561431\n",
      "Train Epoch: 10 [26000/60000 (43%)]\t1.550783\n",
      "Train Epoch: 10 [28000/60000 (47%)]\t1.491865\n",
      "Train Epoch: 10 [30000/60000 (50%)]\t1.504876\n",
      "Train Epoch: 10 [32000/60000 (53%)]\t1.509438\n",
      "Train Epoch: 10 [34000/60000 (57%)]\t1.522803\n",
      "Train Epoch: 10 [36000/60000 (60%)]\t1.535978\n",
      "Train Epoch: 10 [38000/60000 (63%)]\t1.507112\n",
      "Train Epoch: 10 [40000/60000 (67%)]\t1.500695\n",
      "Train Epoch: 10 [42000/60000 (70%)]\t1.551624\n",
      "Train Epoch: 10 [44000/60000 (73%)]\t1.514904\n",
      "Train Epoch: 10 [46000/60000 (77%)]\t1.527171\n",
      "Train Epoch: 10 [48000/60000 (80%)]\t1.533619\n",
      "Train Epoch: 10 [50000/60000 (83%)]\t1.513507\n",
      "Train Epoch: 10 [52000/60000 (87%)]\t1.530958\n",
      "Train Epoch: 10 [54000/60000 (90%)]\t1.518287\n",
      "Train Epoch: 10 [56000/60000 (93%)]\t1.555691\n",
      "Train Epoch: 10 [58000/60000 (97%)]\t1.549081\n",
      "\n",
      "Test set: Average loss 0.0149, Accuracy 9759/10000(98%\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,11):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "\n",
    "data, target = test_data[0]\n",
    "\n",
    "data = data.unsqueeze(0).to(device)\n",
    "\n",
    "output = model(data)\n",
    "\n",
    "prediction = output.argmax(dim=1, keepdim=True).item()\n",
    "\n",
    "print(f'Prediction: {prediction}')\n",
    "\n",
    "image = data.squeeze(0).squeeze(0).cpu().numpy()\n",
    "\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
