{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,\n",
    "    transform = ToTensor(),\n",
    "    download = True\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = False,\n",
    "    transform = ToTensor(),\n",
    "    download = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4,  ..., 5, 6, 8])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.targets # dataset.target shows the lable of the classified data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a DataLoader ?\n",
    "\n",
    "In PyTorch, a DataLoader is a utility class that simplifies the process of loading and iterating over datasets while training deep learning models. Here are some key features of DataLoader:\n",
    "\n",
    "**Batching:** It can divide the dataset into smaller batches, which helps in efficient training.\n",
    "\n",
    "**Shuffling:** It can shuffle the data to ensure that the model does not learn any unintended patterns.\n",
    "\n",
    "**Parallel Processing:** It can load data in parallel using multiple workers, which speeds up the data loading process.\n",
    "\n",
    "**Memory Pinning:** It can pin memory to improve the efficiency of data transfer to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loaders = {\n",
    "    'train': DataLoader(train_data,batch_size=100,shuffle=True,num_workers=1),\n",
    "    'test': DataLoader(test_data,batch_size=100,shuffle=True,num_workers=1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x2b3d74cb560>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x2b3d537dfd0>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is nn.Model ?\n",
    "\n",
    "The nn.Module parameter is actually quite important in PyTorch. When you define a neural network class, it should inherit from nn.Module. This inheritance provides your class with a lot of useful methods and attributes that are essential for building and training neural networks.\n",
    "\n",
    "**Initialization and Structure:** By inheriting from nn.Module, your class gains the ability to register layers (like conv1, conv2, etc.) and parameters. This is crucial for PyTorch to keep track of the model’s parameters and their gradients.\n",
    "\n",
    "**Forward Propagation:** The forward method is a special method in nn.Module that defines the forward pass of the network. When you call your model on an input, it automatically calls the forward method.\n",
    "\n",
    "**Model Management:** Inheriting from nn.Module allows you to use various utility functions provided by PyTorch, such as model.eval(), model.train(), model.to(device), and more. These functions are essential for managing the model’s state and moving it between devices (CPU/GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1,10,kernel_size=5) # 1 input channel， 10 output channel, 5x5 kernel, 10 kernels\n",
    "        self.conv2 = nn.Conv2d(10,20,kernel_size=5) # 20 kernels\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320,50)\n",
    "        self.fc2 = nn.Linear(50,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x),2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)),2))\n",
    "        x = x.view(-1,320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.softmax(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.First Convolutional Layer (conv1):**\n",
    "- Input: 1 channel, 28x28 (assuming MNIST dataset)\n",
    "- Output: 10 channels, 24x24 (since kernel size is 5, the output size is reduced by 4)\n",
    "\n",
    "**2.First Max Pooling Layer:**\n",
    "- Output: 10 channels, 12x12 (pooling with kernel size 2 reduces each dimension by half)\n",
    "\n",
    "**3.Second Convolutional Layer (conv2):**\n",
    "- Input: 10 channels, 12x12\n",
    "- Output: 20 channels, 8x8 (again, kernel size 5 reduces each dimension by 4)\n",
    "\n",
    "**4.Second Max Pooling Layer:**\n",
    "- Output: 20 channels, 4x4 (pooling with kernel size 2 reduces each dimension by half)\n",
    "\n",
    "After these layers, the feature map has dimensions 20 channels x 4 height x 4 width. When you flatten this for the fully connected layer, you get (20 \\times 4 \\times 4 = 320) units.\n",
    "\n",
    "So, the input to the first fully connected layer (fc1) is 320 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2.4.1+cu124\n",
      "12.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_index, (data,target) in enumerate(loaders['train']):\n",
    "        data,target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_index % 20 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_index*len(data)}/{len(loaders[\"train\"].dataset)} ({100.*batch_index/len(loaders[\"train\"]):.0f}%)]\\t{loss.item():.6f}')\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0 \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in loaders['test']:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output,target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(loaders['test'].dataset)\n",
    "    print(f'\\nTest set: Average loss {test_loss:.4f}, Accuracy {correct}/{len(loaders[\"test\"].dataset)}({100.*correct/len(loaders[\"test\"].dataset):.0f}%\\n)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jerry\\AppData\\Local\\Temp\\ipykernel_3288\\2577003603.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\t2.303684\n",
      "Train Epoch: 1 [2000/60000 (3%)]\t2.294044\n",
      "Train Epoch: 1 [4000/60000 (7%)]\t2.212990\n",
      "Train Epoch: 1 [6000/60000 (10%)]\t2.112004\n",
      "Train Epoch: 1 [8000/60000 (13%)]\t1.928479\n",
      "Train Epoch: 1 [10000/60000 (17%)]\t1.938187\n",
      "Train Epoch: 1 [12000/60000 (20%)]\t1.902271\n",
      "Train Epoch: 1 [14000/60000 (23%)]\t1.890950\n",
      "Train Epoch: 1 [16000/60000 (27%)]\t1.775415\n",
      "Train Epoch: 1 [18000/60000 (30%)]\t1.854022\n",
      "Train Epoch: 1 [20000/60000 (33%)]\t1.743333\n",
      "Train Epoch: 1 [22000/60000 (37%)]\t1.717463\n",
      "Train Epoch: 1 [24000/60000 (40%)]\t1.673501\n",
      "Train Epoch: 1 [26000/60000 (43%)]\t1.686477\n",
      "Train Epoch: 1 [28000/60000 (47%)]\t1.693278\n",
      "Train Epoch: 1 [30000/60000 (50%)]\t1.701470\n",
      "Train Epoch: 1 [32000/60000 (53%)]\t1.676902\n",
      "Train Epoch: 1 [34000/60000 (57%)]\t1.649023\n",
      "Train Epoch: 1 [36000/60000 (60%)]\t1.606541\n",
      "Train Epoch: 1 [38000/60000 (63%)]\t1.636781\n",
      "Train Epoch: 1 [40000/60000 (67%)]\t1.617451\n",
      "Train Epoch: 1 [42000/60000 (70%)]\t1.616845\n",
      "Train Epoch: 1 [44000/60000 (73%)]\t1.588206\n",
      "Train Epoch: 1 [46000/60000 (77%)]\t1.630770\n",
      "Train Epoch: 1 [48000/60000 (80%)]\t1.591531\n",
      "Train Epoch: 1 [50000/60000 (83%)]\t1.592576\n",
      "Train Epoch: 1 [52000/60000 (87%)]\t1.623286\n",
      "Train Epoch: 1 [54000/60000 (90%)]\t1.563854\n",
      "Train Epoch: 1 [56000/60000 (93%)]\t1.632204\n",
      "Train Epoch: 1 [58000/60000 (97%)]\t1.576852\n",
      "\n",
      "Test set: Average loss 0.0153, Accuracy 9312/10000(93%\n",
      ")\n",
      "Train Epoch: 2 [0/60000 (0%)]\t1.664485\n",
      "Train Epoch: 2 [2000/60000 (3%)]\t1.617134\n",
      "Train Epoch: 2 [4000/60000 (7%)]\t1.597438\n",
      "Train Epoch: 2 [6000/60000 (10%)]\t1.564471\n",
      "Train Epoch: 2 [8000/60000 (13%)]\t1.576768\n",
      "Train Epoch: 2 [10000/60000 (17%)]\t1.561490\n",
      "Train Epoch: 2 [12000/60000 (20%)]\t1.575990\n",
      "Train Epoch: 2 [14000/60000 (23%)]\t1.583459\n",
      "Train Epoch: 2 [16000/60000 (27%)]\t1.543004\n",
      "Train Epoch: 2 [18000/60000 (30%)]\t1.598958\n",
      "Train Epoch: 2 [20000/60000 (33%)]\t1.578096\n",
      "Train Epoch: 2 [22000/60000 (37%)]\t1.633470\n",
      "Train Epoch: 2 [24000/60000 (40%)]\t1.596803\n",
      "Train Epoch: 2 [26000/60000 (43%)]\t1.579226\n",
      "Train Epoch: 2 [28000/60000 (47%)]\t1.609441\n",
      "Train Epoch: 2 [30000/60000 (50%)]\t1.615226\n",
      "Train Epoch: 2 [32000/60000 (53%)]\t1.569775\n",
      "Train Epoch: 2 [34000/60000 (57%)]\t1.584504\n",
      "Train Epoch: 2 [36000/60000 (60%)]\t1.564276\n",
      "Train Epoch: 2 [38000/60000 (63%)]\t1.594193\n",
      "Train Epoch: 2 [40000/60000 (67%)]\t1.606820\n",
      "Train Epoch: 2 [42000/60000 (70%)]\t1.540652\n",
      "Train Epoch: 2 [44000/60000 (73%)]\t1.590712\n",
      "Train Epoch: 2 [46000/60000 (77%)]\t1.580163\n",
      "Train Epoch: 2 [48000/60000 (80%)]\t1.621866\n",
      "Train Epoch: 2 [50000/60000 (83%)]\t1.602412\n",
      "Train Epoch: 2 [52000/60000 (87%)]\t1.620821\n",
      "Train Epoch: 2 [54000/60000 (90%)]\t1.604433\n",
      "Train Epoch: 2 [56000/60000 (93%)]\t1.546975\n",
      "Train Epoch: 2 [58000/60000 (97%)]\t1.572540\n",
      "\n",
      "Test set: Average loss 0.0151, Accuracy 9525/10000(95%\n",
      ")\n",
      "Train Epoch: 3 [0/60000 (0%)]\t1.576299\n",
      "Train Epoch: 3 [2000/60000 (3%)]\t1.587283\n",
      "Train Epoch: 3 [4000/60000 (7%)]\t1.527547\n",
      "Train Epoch: 3 [6000/60000 (10%)]\t1.566429\n",
      "Train Epoch: 3 [8000/60000 (13%)]\t1.584272\n",
      "Train Epoch: 3 [10000/60000 (17%)]\t1.581553\n",
      "Train Epoch: 3 [12000/60000 (20%)]\t1.587383\n",
      "Train Epoch: 3 [14000/60000 (23%)]\t1.540783\n",
      "Train Epoch: 3 [16000/60000 (27%)]\t1.526847\n",
      "Train Epoch: 3 [18000/60000 (30%)]\t1.558386\n",
      "Train Epoch: 3 [20000/60000 (33%)]\t1.558078\n",
      "Train Epoch: 3 [22000/60000 (37%)]\t1.606968\n",
      "Train Epoch: 3 [24000/60000 (40%)]\t1.554050\n",
      "Train Epoch: 3 [26000/60000 (43%)]\t1.575492\n",
      "Train Epoch: 3 [28000/60000 (47%)]\t1.535702\n",
      "Train Epoch: 3 [30000/60000 (50%)]\t1.541759\n",
      "Train Epoch: 3 [32000/60000 (53%)]\t1.537160\n",
      "Train Epoch: 3 [34000/60000 (57%)]\t1.588446\n",
      "Train Epoch: 3 [36000/60000 (60%)]\t1.606395\n",
      "Train Epoch: 3 [38000/60000 (63%)]\t1.549002\n",
      "Train Epoch: 3 [40000/60000 (67%)]\t1.539933\n",
      "Train Epoch: 3 [42000/60000 (70%)]\t1.579468\n",
      "Train Epoch: 3 [44000/60000 (73%)]\t1.559247\n",
      "Train Epoch: 3 [46000/60000 (77%)]\t1.529704\n",
      "Train Epoch: 3 [48000/60000 (80%)]\t1.566527\n",
      "Train Epoch: 3 [50000/60000 (83%)]\t1.556881\n",
      "Train Epoch: 3 [52000/60000 (87%)]\t1.601197\n",
      "Train Epoch: 3 [54000/60000 (90%)]\t1.576033\n",
      "Train Epoch: 3 [56000/60000 (93%)]\t1.538860\n",
      "Train Epoch: 3 [58000/60000 (97%)]\t1.548500\n",
      "\n",
      "Test set: Average loss 0.0150, Accuracy 9579/10000(96%\n",
      ")\n",
      "Train Epoch: 4 [0/60000 (0%)]\t1.520726\n",
      "Train Epoch: 4 [2000/60000 (3%)]\t1.570042\n",
      "Train Epoch: 4 [4000/60000 (7%)]\t1.574304\n",
      "Train Epoch: 4 [6000/60000 (10%)]\t1.558263\n",
      "Train Epoch: 4 [8000/60000 (13%)]\t1.579282\n",
      "Train Epoch: 4 [10000/60000 (17%)]\t1.565419\n",
      "Train Epoch: 4 [12000/60000 (20%)]\t1.571726\n",
      "Train Epoch: 4 [14000/60000 (23%)]\t1.513420\n",
      "Train Epoch: 4 [16000/60000 (27%)]\t1.521304\n",
      "Train Epoch: 4 [18000/60000 (30%)]\t1.543341\n",
      "Train Epoch: 4 [20000/60000 (33%)]\t1.580350\n",
      "Train Epoch: 4 [22000/60000 (37%)]\t1.549861\n",
      "Train Epoch: 4 [24000/60000 (40%)]\t1.520171\n",
      "Train Epoch: 4 [26000/60000 (43%)]\t1.529941\n",
      "Train Epoch: 4 [28000/60000 (47%)]\t1.538178\n",
      "Train Epoch: 4 [30000/60000 (50%)]\t1.536271\n",
      "Train Epoch: 4 [32000/60000 (53%)]\t1.575279\n",
      "Train Epoch: 4 [34000/60000 (57%)]\t1.577559\n",
      "Train Epoch: 4 [36000/60000 (60%)]\t1.541117\n",
      "Train Epoch: 4 [38000/60000 (63%)]\t1.540076\n",
      "Train Epoch: 4 [40000/60000 (67%)]\t1.586858\n",
      "Train Epoch: 4 [42000/60000 (70%)]\t1.564966\n",
      "Train Epoch: 4 [44000/60000 (73%)]\t1.552673\n",
      "Train Epoch: 4 [46000/60000 (77%)]\t1.562617\n",
      "Train Epoch: 4 [48000/60000 (80%)]\t1.565405\n",
      "Train Epoch: 4 [50000/60000 (83%)]\t1.592468\n",
      "Train Epoch: 4 [52000/60000 (87%)]\t1.507436\n",
      "Train Epoch: 4 [54000/60000 (90%)]\t1.541432\n",
      "Train Epoch: 4 [56000/60000 (93%)]\t1.505585\n",
      "Train Epoch: 4 [58000/60000 (97%)]\t1.517117\n",
      "\n",
      "Test set: Average loss 0.0150, Accuracy 9644/10000(96%\n",
      ")\n",
      "Train Epoch: 5 [0/60000 (0%)]\t1.555319\n",
      "Train Epoch: 5 [2000/60000 (3%)]\t1.559695\n",
      "Train Epoch: 5 [4000/60000 (7%)]\t1.548934\n",
      "Train Epoch: 5 [6000/60000 (10%)]\t1.519197\n",
      "Train Epoch: 5 [8000/60000 (13%)]\t1.561369\n",
      "Train Epoch: 5 [10000/60000 (17%)]\t1.514424\n",
      "Train Epoch: 5 [12000/60000 (20%)]\t1.552053\n",
      "Train Epoch: 5 [14000/60000 (23%)]\t1.530041\n",
      "Train Epoch: 5 [16000/60000 (27%)]\t1.576162\n",
      "Train Epoch: 5 [18000/60000 (30%)]\t1.535583\n",
      "Train Epoch: 5 [20000/60000 (33%)]\t1.578616\n",
      "Train Epoch: 5 [22000/60000 (37%)]\t1.518123\n",
      "Train Epoch: 5 [24000/60000 (40%)]\t1.521770\n",
      "Train Epoch: 5 [26000/60000 (43%)]\t1.573470\n",
      "Train Epoch: 5 [28000/60000 (47%)]\t1.526598\n",
      "Train Epoch: 5 [30000/60000 (50%)]\t1.517237\n",
      "Train Epoch: 5 [32000/60000 (53%)]\t1.571946\n",
      "Train Epoch: 5 [34000/60000 (57%)]\t1.549048\n",
      "Train Epoch: 5 [36000/60000 (60%)]\t1.530523\n",
      "Train Epoch: 5 [38000/60000 (63%)]\t1.549226\n",
      "Train Epoch: 5 [40000/60000 (67%)]\t1.521582\n",
      "Train Epoch: 5 [42000/60000 (70%)]\t1.502694\n",
      "Train Epoch: 5 [44000/60000 (73%)]\t1.594989\n",
      "Train Epoch: 5 [46000/60000 (77%)]\t1.574000\n",
      "Train Epoch: 5 [48000/60000 (80%)]\t1.540517\n",
      "Train Epoch: 5 [50000/60000 (83%)]\t1.564649\n",
      "Train Epoch: 5 [52000/60000 (87%)]\t1.539029\n",
      "Train Epoch: 5 [54000/60000 (90%)]\t1.520325\n",
      "Train Epoch: 5 [56000/60000 (93%)]\t1.571410\n",
      "Train Epoch: 5 [58000/60000 (97%)]\t1.526470\n",
      "\n",
      "Test set: Average loss 0.0149, Accuracy 9671/10000(97%\n",
      ")\n",
      "Train Epoch: 6 [0/60000 (0%)]\t1.533454\n",
      "Train Epoch: 6 [2000/60000 (3%)]\t1.557137\n",
      "Train Epoch: 6 [4000/60000 (7%)]\t1.571657\n",
      "Train Epoch: 6 [6000/60000 (10%)]\t1.571631\n",
      "Train Epoch: 6 [8000/60000 (13%)]\t1.550923\n",
      "Train Epoch: 6 [10000/60000 (17%)]\t1.548488\n",
      "Train Epoch: 6 [12000/60000 (20%)]\t1.556386\n",
      "Train Epoch: 6 [14000/60000 (23%)]\t1.557162\n",
      "Train Epoch: 6 [16000/60000 (27%)]\t1.603366\n",
      "Train Epoch: 6 [18000/60000 (30%)]\t1.539373\n",
      "Train Epoch: 6 [20000/60000 (33%)]\t1.539883\n",
      "Train Epoch: 6 [22000/60000 (37%)]\t1.530782\n",
      "Train Epoch: 6 [24000/60000 (40%)]\t1.528368\n",
      "Train Epoch: 6 [26000/60000 (43%)]\t1.560144\n",
      "Train Epoch: 6 [28000/60000 (47%)]\t1.532357\n",
      "Train Epoch: 6 [30000/60000 (50%)]\t1.530029\n",
      "Train Epoch: 6 [32000/60000 (53%)]\t1.552022\n",
      "Train Epoch: 6 [34000/60000 (57%)]\t1.498317\n",
      "Train Epoch: 6 [36000/60000 (60%)]\t1.568601\n",
      "Train Epoch: 6 [38000/60000 (63%)]\t1.534083\n",
      "Train Epoch: 6 [40000/60000 (67%)]\t1.533221\n",
      "Train Epoch: 6 [42000/60000 (70%)]\t1.563929\n",
      "Train Epoch: 6 [44000/60000 (73%)]\t1.511521\n",
      "Train Epoch: 6 [46000/60000 (77%)]\t1.556790\n",
      "Train Epoch: 6 [48000/60000 (80%)]\t1.549423\n",
      "Train Epoch: 6 [50000/60000 (83%)]\t1.498582\n",
      "Train Epoch: 6 [52000/60000 (87%)]\t1.534099\n",
      "Train Epoch: 6 [54000/60000 (90%)]\t1.537285\n",
      "Train Epoch: 6 [56000/60000 (93%)]\t1.512748\n",
      "Train Epoch: 6 [58000/60000 (97%)]\t1.541543\n",
      "\n",
      "Test set: Average loss 0.0149, Accuracy 9673/10000(97%\n",
      ")\n",
      "Train Epoch: 7 [0/60000 (0%)]\t1.532270\n",
      "Train Epoch: 7 [2000/60000 (3%)]\t1.556537\n",
      "Train Epoch: 7 [4000/60000 (7%)]\t1.565699\n",
      "Train Epoch: 7 [6000/60000 (10%)]\t1.540175\n",
      "Train Epoch: 7 [8000/60000 (13%)]\t1.516007\n",
      "Train Epoch: 7 [10000/60000 (17%)]\t1.575050\n",
      "Train Epoch: 7 [12000/60000 (20%)]\t1.503193\n",
      "Train Epoch: 7 [14000/60000 (23%)]\t1.625345\n",
      "Train Epoch: 7 [16000/60000 (27%)]\t1.558935\n",
      "Train Epoch: 7 [18000/60000 (30%)]\t1.515924\n",
      "Train Epoch: 7 [20000/60000 (33%)]\t1.560088\n",
      "Train Epoch: 7 [22000/60000 (37%)]\t1.537527\n",
      "Train Epoch: 7 [24000/60000 (40%)]\t1.534407\n",
      "Train Epoch: 7 [26000/60000 (43%)]\t1.572300\n",
      "Train Epoch: 7 [28000/60000 (47%)]\t1.540879\n",
      "Train Epoch: 7 [30000/60000 (50%)]\t1.519625\n",
      "Train Epoch: 7 [32000/60000 (53%)]\t1.507829\n",
      "Train Epoch: 7 [34000/60000 (57%)]\t1.546917\n",
      "Train Epoch: 7 [36000/60000 (60%)]\t1.568640\n",
      "Train Epoch: 7 [38000/60000 (63%)]\t1.530023\n",
      "Train Epoch: 7 [40000/60000 (67%)]\t1.559555\n",
      "Train Epoch: 7 [42000/60000 (70%)]\t1.500227\n",
      "Train Epoch: 7 [44000/60000 (73%)]\t1.538211\n",
      "Train Epoch: 7 [46000/60000 (77%)]\t1.582839\n",
      "Train Epoch: 7 [48000/60000 (80%)]\t1.569781\n",
      "Train Epoch: 7 [50000/60000 (83%)]\t1.564164\n",
      "Train Epoch: 7 [52000/60000 (87%)]\t1.526824\n",
      "Train Epoch: 7 [54000/60000 (90%)]\t1.531644\n",
      "Train Epoch: 7 [56000/60000 (93%)]\t1.582170\n",
      "Train Epoch: 7 [58000/60000 (97%)]\t1.538129\n",
      "\n",
      "Test set: Average loss 0.0149, Accuracy 9692/10000(97%\n",
      ")\n",
      "Train Epoch: 8 [0/60000 (0%)]\t1.579512\n",
      "Train Epoch: 8 [2000/60000 (3%)]\t1.518940\n",
      "Train Epoch: 8 [4000/60000 (7%)]\t1.553935\n",
      "Train Epoch: 8 [6000/60000 (10%)]\t1.543755\n",
      "Train Epoch: 8 [8000/60000 (13%)]\t1.571072\n",
      "Train Epoch: 8 [10000/60000 (17%)]\t1.493839\n",
      "Train Epoch: 8 [12000/60000 (20%)]\t1.510291\n",
      "Train Epoch: 8 [14000/60000 (23%)]\t1.563944\n",
      "Train Epoch: 8 [16000/60000 (27%)]\t1.534750\n",
      "Train Epoch: 8 [18000/60000 (30%)]\t1.535730\n",
      "Train Epoch: 8 [20000/60000 (33%)]\t1.493031\n",
      "Train Epoch: 8 [22000/60000 (37%)]\t1.512431\n",
      "Train Epoch: 8 [24000/60000 (40%)]\t1.545013\n",
      "Train Epoch: 8 [26000/60000 (43%)]\t1.509162\n",
      "Train Epoch: 8 [28000/60000 (47%)]\t1.531115\n",
      "Train Epoch: 8 [30000/60000 (50%)]\t1.567124\n",
      "Train Epoch: 8 [32000/60000 (53%)]\t1.537741\n",
      "Train Epoch: 8 [34000/60000 (57%)]\t1.513548\n",
      "Train Epoch: 8 [36000/60000 (60%)]\t1.506912\n",
      "Train Epoch: 8 [38000/60000 (63%)]\t1.532581\n",
      "Train Epoch: 8 [40000/60000 (67%)]\t1.532944\n",
      "Train Epoch: 8 [42000/60000 (70%)]\t1.517972\n",
      "Train Epoch: 8 [44000/60000 (73%)]\t1.515019\n",
      "Train Epoch: 8 [46000/60000 (77%)]\t1.590578\n",
      "Train Epoch: 8 [48000/60000 (80%)]\t1.491373\n",
      "Train Epoch: 8 [50000/60000 (83%)]\t1.506609\n",
      "Train Epoch: 8 [52000/60000 (87%)]\t1.537192\n",
      "Train Epoch: 8 [54000/60000 (90%)]\t1.557076\n",
      "Train Epoch: 8 [56000/60000 (93%)]\t1.534970\n",
      "Train Epoch: 8 [58000/60000 (97%)]\t1.516741\n",
      "\n",
      "Test set: Average loss 0.0149, Accuracy 9694/10000(97%\n",
      ")\n",
      "Train Epoch: 9 [0/60000 (0%)]\t1.518233\n",
      "Train Epoch: 9 [2000/60000 (3%)]\t1.540540\n",
      "Train Epoch: 9 [4000/60000 (7%)]\t1.519521\n",
      "Train Epoch: 9 [6000/60000 (10%)]\t1.530820\n",
      "Train Epoch: 9 [8000/60000 (13%)]\t1.546122\n",
      "Train Epoch: 9 [10000/60000 (17%)]\t1.503258\n",
      "Train Epoch: 9 [12000/60000 (20%)]\t1.534813\n",
      "Train Epoch: 9 [14000/60000 (23%)]\t1.504012\n",
      "Train Epoch: 9 [16000/60000 (27%)]\t1.534615\n",
      "Train Epoch: 9 [18000/60000 (30%)]\t1.542241\n",
      "Train Epoch: 9 [20000/60000 (33%)]\t1.527217\n",
      "Train Epoch: 9 [22000/60000 (37%)]\t1.506663\n",
      "Train Epoch: 9 [24000/60000 (40%)]\t1.561631\n",
      "Train Epoch: 9 [26000/60000 (43%)]\t1.537314\n",
      "Train Epoch: 9 [28000/60000 (47%)]\t1.505724\n",
      "Train Epoch: 9 [30000/60000 (50%)]\t1.551799\n",
      "Train Epoch: 9 [32000/60000 (53%)]\t1.578325\n",
      "Train Epoch: 9 [34000/60000 (57%)]\t1.550779\n",
      "Train Epoch: 9 [36000/60000 (60%)]\t1.605763\n",
      "Train Epoch: 9 [38000/60000 (63%)]\t1.541322\n",
      "Train Epoch: 9 [40000/60000 (67%)]\t1.544118\n",
      "Train Epoch: 9 [42000/60000 (70%)]\t1.547162\n",
      "Train Epoch: 9 [44000/60000 (73%)]\t1.551078\n",
      "Train Epoch: 9 [46000/60000 (77%)]\t1.564066\n",
      "Train Epoch: 9 [48000/60000 (80%)]\t1.534016\n",
      "Train Epoch: 9 [50000/60000 (83%)]\t1.515798\n",
      "Train Epoch: 9 [52000/60000 (87%)]\t1.545456\n",
      "Train Epoch: 9 [54000/60000 (90%)]\t1.530424\n",
      "Train Epoch: 9 [56000/60000 (93%)]\t1.531762\n",
      "Train Epoch: 9 [58000/60000 (97%)]\t1.505363\n",
      "\n",
      "Test set: Average loss 0.0149, Accuracy 9677/10000(97%\n",
      ")\n",
      "Train Epoch: 10 [0/60000 (0%)]\t1.545266\n",
      "Train Epoch: 10 [2000/60000 (3%)]\t1.570641\n",
      "Train Epoch: 10 [4000/60000 (7%)]\t1.558099\n",
      "Train Epoch: 10 [6000/60000 (10%)]\t1.519102\n",
      "Train Epoch: 10 [8000/60000 (13%)]\t1.528846\n",
      "Train Epoch: 10 [10000/60000 (17%)]\t1.555998\n",
      "Train Epoch: 10 [12000/60000 (20%)]\t1.513307\n",
      "Train Epoch: 10 [14000/60000 (23%)]\t1.587244\n",
      "Train Epoch: 10 [16000/60000 (27%)]\t1.532131\n",
      "Train Epoch: 10 [18000/60000 (30%)]\t1.542229\n",
      "Train Epoch: 10 [20000/60000 (33%)]\t1.553850\n",
      "Train Epoch: 10 [22000/60000 (37%)]\t1.544077\n",
      "Train Epoch: 10 [24000/60000 (40%)]\t1.559652\n",
      "Train Epoch: 10 [26000/60000 (43%)]\t1.543124\n",
      "Train Epoch: 10 [28000/60000 (47%)]\t1.588217\n",
      "Train Epoch: 10 [30000/60000 (50%)]\t1.482167\n",
      "Train Epoch: 10 [32000/60000 (53%)]\t1.537823\n",
      "Train Epoch: 10 [34000/60000 (57%)]\t1.570305\n",
      "Train Epoch: 10 [36000/60000 (60%)]\t1.551475\n",
      "Train Epoch: 10 [38000/60000 (63%)]\t1.543380\n",
      "Train Epoch: 10 [40000/60000 (67%)]\t1.540352\n",
      "Train Epoch: 10 [42000/60000 (70%)]\t1.555228\n",
      "Train Epoch: 10 [44000/60000 (73%)]\t1.556019\n",
      "Train Epoch: 10 [46000/60000 (77%)]\t1.520471\n",
      "Train Epoch: 10 [48000/60000 (80%)]\t1.535282\n",
      "Train Epoch: 10 [50000/60000 (83%)]\t1.569466\n",
      "Train Epoch: 10 [52000/60000 (87%)]\t1.536602\n",
      "Train Epoch: 10 [54000/60000 (90%)]\t1.546839\n",
      "Train Epoch: 10 [56000/60000 (93%)]\t1.512957\n",
      "Train Epoch: 10 [58000/60000 (97%)]\t1.521463\n",
      "\n",
      "Test set: Average loss 0.0149, Accuracy 9737/10000(97%\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,11):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jerry\\AppData\\Local\\Temp\\ipykernel_3288\\2577003603.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "\n",
    "data, target = test_data[0]\n",
    "\n",
    "data = data.unsqueeze(0).to(device)\n",
    "\n",
    "output = model(data)\n",
    "\n",
    "prediction = output.argmax(dim=1, keepdim=True).item()\n",
    "\n",
    "print(f'Prediction: {prediction}')\n",
    "\n",
    "image = data.squeeze(0).squeeze(0).cpu().numpy()\n",
    "\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
